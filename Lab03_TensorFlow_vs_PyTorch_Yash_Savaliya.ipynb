{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yash-Savaliya-242/Tensorflow-and-PyTorch-lab03-/blob/main/Lab03_TensorFlow_vs_PyTorch_Yash_Savaliya.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c1e84a1",
      "metadata": {
        "id": "5c1e84a1"
      },
      "source": [
        "# Lab 03: TensorFlow vs. PyTorch\n",
        "- Train a model on MNIST in both TensorFlow and PyTorch, convert to TFLite and ONNX.  \n",
        "- Use tf.GradientTape for Tensorflow custom training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c58bba",
      "metadata": {
        "id": "a1c58bba"
      },
      "source": [
        "## TensorFlow Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "23ebc05e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23ebc05e",
        "outputId": "edd25697-a59c-469d-c7a4-7785c6205a98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.8845 - loss: 0.4287\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9642 - loss: 0.1225\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9770 - loss: 0.0782\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9829 - loss: 0.0569\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9877 - loss: 0.0418\n",
            "TF Training time: 49.20 seconds\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9732 - loss: 0.0925\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08076023310422897, 0.9757999777793884]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Normalize pixel values\n",
        "x_test = x_test / 255.0\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),        # MNIST image shape\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),  # Hidden layer with 128 neurons\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # Output layer for 10 classes\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',       # Multi-class classification loss\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "start = time.time()\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "end = time.time()\n",
        "print(f\"TF Training time: {end - start:.2f} seconds\")\n",
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72743ab8",
      "metadata": {
        "id": "72743ab8"
      },
      "source": [
        "## Convert TensorFlow model to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "be6ab50a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be6ab50a",
        "outputId": "fa2964c5-ab0b-4d64-cb08-1540d2c88227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpv_6rukwi'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136739725438096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136739725442896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136739725441744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136739725438672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c00c95",
      "metadata": {
        "id": "57c00c95"
      },
      "source": [
        "## PyTorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "623dfb49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "623dfb49",
        "outputId": "2b66fb66-5b67-49b9-99b1-91bea3b5bce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 58.6MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.70MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.8MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.43MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Training time: 64.67 seconds\n",
            "Test accuracy: 0.9723\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "# Data loaders\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "train_loader = DataLoader(datasets.MNIST(root='./data', train=True, transform=transform, download=True), batch_size=32)\n",
        "test_loader = DataLoader(datasets.MNIST(root='./data', train=False, transform=transform, download=True), batch_size=1000)\n",
        "\n",
        "# Model definition\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "start = time.time()\n",
        "for epoch in range(5):\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "end = time.time()\n",
        "print(f\"PyTorch Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        output = model(x)\n",
        "        pred = output.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "print(f\"Test accuracy: {correct / len(test_loader.dataset):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6dbdab0",
      "metadata": {
        "id": "f6dbdab0"
      },
      "source": [
        "## Convert PyTorch model to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "WuMKMhHc8aLF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuMKMhHc8aLF",
        "outputId": "bec0699a-0196-4160-f569-95abf987030b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n"
          ]
        }
      ],
      "source": [
        "# Install ONNX\n",
        "!pip install onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "09925e9a",
      "metadata": {
        "id": "09925e9a"
      },
      "outputs": [],
      "source": [
        "dummy_input = torch.randn(1, 784)\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Xk39IA9S4XL"
      },
      "id": "9Xk39IA9S4XL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "sv4P-dSS_GQB",
      "metadata": {
        "id": "sv4P-dSS_GQB"
      },
      "source": [
        "## TensorFlow custom training loop using tf.GradientTape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "KH-sDlHq_Gdw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH-sDlHq_Gdw",
        "outputId": "05197045-d49b-4f3f-850f-9f7f3710aca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.3148, Accuracy: 0.1875\n",
            "Step 100, Loss: 0.3813, Accuracy: 0.7710\n",
            "Step 200, Loss: 0.3963, Accuracy: 0.8291\n",
            "Step 300, Loss: 0.2935, Accuracy: 0.8543\n",
            "Step 400, Loss: 0.2359, Accuracy: 0.8699\n",
            "Step 500, Loss: 0.6055, Accuracy: 0.8803\n",
            "Step 600, Loss: 0.1140, Accuracy: 0.8887\n",
            "Step 700, Loss: 0.2442, Accuracy: 0.8947\n",
            "Step 800, Loss: 0.1722, Accuracy: 0.8999\n",
            "Step 900, Loss: 0.1052, Accuracy: 0.9045\n",
            "Step 1000, Loss: 0.0741, Accuracy: 0.9082\n",
            "Step 1100, Loss: 0.1825, Accuracy: 0.9108\n",
            "Step 1200, Loss: 0.1734, Accuracy: 0.9138\n",
            "Step 1300, Loss: 0.5663, Accuracy: 0.9163\n",
            "Step 1400, Loss: 0.1533, Accuracy: 0.9184\n",
            "Step 1500, Loss: 0.1344, Accuracy: 0.9207\n",
            "Step 1600, Loss: 0.1593, Accuracy: 0.9230\n",
            "Step 1700, Loss: 0.4607, Accuracy: 0.9245\n",
            "Step 1800, Loss: 0.1223, Accuracy: 0.9268\n",
            "Training Accuracy for epoch 1: 0.9281\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.1387, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.1925, Accuracy: 0.9570\n",
            "Step 200, Loss: 0.0468, Accuracy: 0.9613\n",
            "Step 300, Loss: 0.0631, Accuracy: 0.9615\n",
            "Step 400, Loss: 0.1589, Accuracy: 0.9628\n",
            "Step 500, Loss: 0.0975, Accuracy: 0.9622\n",
            "Step 600, Loss: 0.0199, Accuracy: 0.9627\n",
            "Step 700, Loss: 0.0339, Accuracy: 0.9634\n",
            "Step 800, Loss: 0.0578, Accuracy: 0.9640\n",
            "Step 900, Loss: 0.2760, Accuracy: 0.9641\n",
            "Step 1000, Loss: 0.0540, Accuracy: 0.9645\n",
            "Step 1100, Loss: 0.0509, Accuracy: 0.9650\n",
            "Step 1200, Loss: 0.1705, Accuracy: 0.9653\n",
            "Step 1300, Loss: 0.0413, Accuracy: 0.9656\n",
            "Step 1400, Loss: 0.0916, Accuracy: 0.9656\n",
            "Step 1500, Loss: 0.1184, Accuracy: 0.9658\n",
            "Step 1600, Loss: 0.0982, Accuracy: 0.9662\n",
            "Step 1700, Loss: 0.0654, Accuracy: 0.9667\n",
            "Step 1800, Loss: 0.4166, Accuracy: 0.9672\n",
            "Training Accuracy for epoch 2: 0.9675\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.1334, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0973, Accuracy: 0.9759\n",
            "Step 200, Loss: 0.0600, Accuracy: 0.9736\n",
            "Step 300, Loss: 0.0315, Accuracy: 0.9736\n",
            "Step 400, Loss: 0.0599, Accuracy: 0.9745\n",
            "Step 500, Loss: 0.0718, Accuracy: 0.9756\n",
            "Step 600, Loss: 0.0391, Accuracy: 0.9755\n",
            "Step 700, Loss: 0.0145, Accuracy: 0.9754\n",
            "Step 800, Loss: 0.0512, Accuracy: 0.9763\n",
            "Step 900, Loss: 0.1691, Accuracy: 0.9769\n",
            "Step 1000, Loss: 0.0136, Accuracy: 0.9771\n",
            "Step 1100, Loss: 0.0106, Accuracy: 0.9774\n",
            "Step 1200, Loss: 0.0389, Accuracy: 0.9772\n",
            "Step 1300, Loss: 0.0824, Accuracy: 0.9771\n",
            "Step 1400, Loss: 0.0087, Accuracy: 0.9768\n",
            "Step 1500, Loss: 0.0931, Accuracy: 0.9766\n",
            "Step 1600, Loss: 0.0112, Accuracy: 0.9771\n",
            "Step 1700, Loss: 0.0416, Accuracy: 0.9773\n",
            "Step 1800, Loss: 0.0214, Accuracy: 0.9776\n",
            "Training Accuracy for epoch 3: 0.9776\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.1681, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.1059, Accuracy: 0.9787\n",
            "Step 200, Loss: 0.0245, Accuracy: 0.9817\n",
            "Step 300, Loss: 0.1151, Accuracy: 0.9818\n",
            "Step 400, Loss: 0.0332, Accuracy: 0.9822\n",
            "Step 500, Loss: 0.0175, Accuracy: 0.9825\n",
            "Step 600, Loss: 0.0079, Accuracy: 0.9832\n",
            "Step 700, Loss: 0.0651, Accuracy: 0.9826\n",
            "Step 800, Loss: 0.0224, Accuracy: 0.9828\n",
            "Step 900, Loss: 0.0682, Accuracy: 0.9824\n",
            "Step 1000, Loss: 0.0108, Accuracy: 0.9824\n",
            "Step 1100, Loss: 0.0742, Accuracy: 0.9824\n",
            "Step 1200, Loss: 0.0362, Accuracy: 0.9824\n",
            "Step 1300, Loss: 0.0150, Accuracy: 0.9823\n",
            "Step 1400, Loss: 0.0099, Accuracy: 0.9825\n",
            "Step 1500, Loss: 0.0063, Accuracy: 0.9827\n",
            "Step 1600, Loss: 0.0081, Accuracy: 0.9827\n",
            "Step 1700, Loss: 0.0365, Accuracy: 0.9828\n",
            "Step 1800, Loss: 0.0654, Accuracy: 0.9827\n",
            "Training Accuracy for epoch 4: 0.9826\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0766, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0837, Accuracy: 0.9901\n",
            "Step 200, Loss: 0.0014, Accuracy: 0.9877\n",
            "Step 300, Loss: 0.0220, Accuracy: 0.9881\n",
            "Step 400, Loss: 0.0077, Accuracy: 0.9889\n",
            "Step 500, Loss: 0.0245, Accuracy: 0.9884\n",
            "Step 600, Loss: 0.0903, Accuracy: 0.9874\n",
            "Step 700, Loss: 0.0118, Accuracy: 0.9878\n",
            "Step 800, Loss: 0.0038, Accuracy: 0.9879\n",
            "Step 900, Loss: 0.1786, Accuracy: 0.9880\n",
            "Step 1000, Loss: 0.0174, Accuracy: 0.9875\n",
            "Step 1100, Loss: 0.1126, Accuracy: 0.9874\n",
            "Step 1200, Loss: 0.0159, Accuracy: 0.9877\n",
            "Step 1300, Loss: 0.0144, Accuracy: 0.9872\n",
            "Step 1400, Loss: 0.0259, Accuracy: 0.9871\n",
            "Step 1500, Loss: 0.0608, Accuracy: 0.9870\n",
            "Step 1600, Loss: 0.0223, Accuracy: 0.9868\n",
            "Step 1700, Loss: 0.0020, Accuracy: 0.9869\n",
            "Step 1800, Loss: 0.0622, Accuracy: 0.9872\n",
            "Training Accuracy for epoch 5: 0.9872\n",
            "\n",
            "TF Training time: 393.90 seconds\n",
            "Test Accuracy: 0.9766\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Fill in normalization factor\n",
        "x_test = x_test / 255.0   # Fill in normalization factor\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32         # Fill same batch size as in first TF example\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),    # Fill size\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),    # Fill number of neurons and activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Fill number of neurons and activation\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        train_acc_metric.update_state(y_batch, logits)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E4Nlg4lb_qdW",
      "metadata": {
        "id": "E4Nlg4lb_qdW"
      },
      "source": [
        "## Performance Otimization with Graph Execution using @tf.function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "Jmu_hciK_qle",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmu_hciK_qle",
        "outputId": "9114a550-9404-41ea-93ed-365241c7a3cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.4893, Accuracy: 0.1250\n",
            "Step 100, Loss: 0.4617, Accuracy: 0.7559\n",
            "Step 200, Loss: 0.5233, Accuracy: 0.8186\n",
            "Step 300, Loss: 0.1939, Accuracy: 0.8491\n",
            "Step 400, Loss: 0.2252, Accuracy: 0.8660\n",
            "Step 500, Loss: 0.1000, Accuracy: 0.8765\n",
            "Step 600, Loss: 0.1967, Accuracy: 0.8852\n",
            "Step 700, Loss: 0.3650, Accuracy: 0.8920\n",
            "Step 800, Loss: 0.1205, Accuracy: 0.8971\n",
            "Step 900, Loss: 0.1424, Accuracy: 0.9015\n",
            "Step 1000, Loss: 0.2553, Accuracy: 0.9054\n",
            "Step 1100, Loss: 0.0705, Accuracy: 0.9087\n",
            "Step 1200, Loss: 0.1668, Accuracy: 0.9115\n",
            "Step 1300, Loss: 0.5505, Accuracy: 0.9138\n",
            "Step 1400, Loss: 0.1903, Accuracy: 0.9161\n",
            "Step 1500, Loss: 0.2177, Accuracy: 0.9185\n",
            "Step 1600, Loss: 0.1007, Accuracy: 0.9209\n",
            "Step 1700, Loss: 0.0800, Accuracy: 0.9235\n",
            "Step 1800, Loss: 0.1221, Accuracy: 0.9254\n",
            "Training Accuracy for epoch 1: 0.9267\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.0487, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.1055, Accuracy: 0.9638\n",
            "Step 200, Loss: 0.2141, Accuracy: 0.9616\n",
            "Step 300, Loss: 0.1869, Accuracy: 0.9623\n",
            "Step 400, Loss: 0.0622, Accuracy: 0.9613\n",
            "Step 500, Loss: 0.1871, Accuracy: 0.9611\n",
            "Step 600, Loss: 0.2182, Accuracy: 0.9606\n",
            "Step 700, Loss: 0.1559, Accuracy: 0.9613\n",
            "Step 800, Loss: 0.1242, Accuracy: 0.9617\n",
            "Step 900, Loss: 0.0682, Accuracy: 0.9619\n",
            "Step 1000, Loss: 0.0220, Accuracy: 0.9627\n",
            "Step 1100, Loss: 0.0392, Accuracy: 0.9631\n",
            "Step 1200, Loss: 0.1421, Accuracy: 0.9635\n",
            "Step 1300, Loss: 0.1195, Accuracy: 0.9631\n",
            "Step 1400, Loss: 0.1467, Accuracy: 0.9634\n",
            "Step 1500, Loss: 0.0235, Accuracy: 0.9634\n",
            "Step 1600, Loss: 0.0726, Accuracy: 0.9641\n",
            "Step 1700, Loss: 0.0242, Accuracy: 0.9646\n",
            "Step 1800, Loss: 0.2939, Accuracy: 0.9648\n",
            "Training Accuracy for epoch 2: 0.9653\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.0368, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0253, Accuracy: 0.9746\n",
            "Step 200, Loss: 0.2770, Accuracy: 0.9756\n",
            "Step 300, Loss: 0.1331, Accuracy: 0.9767\n",
            "Step 400, Loss: 0.1819, Accuracy: 0.9760\n",
            "Step 500, Loss: 0.2558, Accuracy: 0.9752\n",
            "Step 600, Loss: 0.0552, Accuracy: 0.9749\n",
            "Step 700, Loss: 0.0417, Accuracy: 0.9750\n",
            "Step 800, Loss: 0.2462, Accuracy: 0.9750\n",
            "Step 900, Loss: 0.1605, Accuracy: 0.9749\n",
            "Step 1000, Loss: 0.0391, Accuracy: 0.9753\n",
            "Step 1100, Loss: 0.1499, Accuracy: 0.9752\n",
            "Step 1200, Loss: 0.0466, Accuracy: 0.9749\n",
            "Step 1300, Loss: 0.1175, Accuracy: 0.9748\n",
            "Step 1400, Loss: 0.0352, Accuracy: 0.9753\n",
            "Step 1500, Loss: 0.0424, Accuracy: 0.9752\n",
            "Step 1600, Loss: 0.1818, Accuracy: 0.9754\n",
            "Step 1700, Loss: 0.0187, Accuracy: 0.9757\n",
            "Step 1800, Loss: 0.0902, Accuracy: 0.9759\n",
            "Training Accuracy for epoch 3: 0.9761\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0382, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0069, Accuracy: 0.9824\n",
            "Step 200, Loss: 0.0101, Accuracy: 0.9821\n",
            "Step 300, Loss: 0.0290, Accuracy: 0.9820\n",
            "Step 400, Loss: 0.0599, Accuracy: 0.9824\n",
            "Step 500, Loss: 0.0162, Accuracy: 0.9825\n",
            "Step 600, Loss: 0.0456, Accuracy: 0.9819\n",
            "Step 700, Loss: 0.2178, Accuracy: 0.9818\n",
            "Step 800, Loss: 0.0671, Accuracy: 0.9819\n",
            "Step 900, Loss: 0.0187, Accuracy: 0.9820\n",
            "Step 1000, Loss: 0.0211, Accuracy: 0.9821\n",
            "Step 1100, Loss: 0.1044, Accuracy: 0.9818\n",
            "Step 1200, Loss: 0.0193, Accuracy: 0.9814\n",
            "Step 1300, Loss: 0.0184, Accuracy: 0.9813\n",
            "Step 1400, Loss: 0.0661, Accuracy: 0.9812\n",
            "Step 1500, Loss: 0.0490, Accuracy: 0.9810\n",
            "Step 1600, Loss: 0.0196, Accuracy: 0.9814\n",
            "Step 1700, Loss: 0.1065, Accuracy: 0.9813\n",
            "Step 1800, Loss: 0.0443, Accuracy: 0.9815\n",
            "Training Accuracy for epoch 4: 0.9816\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0027, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0047, Accuracy: 0.9879\n",
            "Step 200, Loss: 0.0211, Accuracy: 0.9871\n",
            "Step 300, Loss: 0.0055, Accuracy: 0.9862\n",
            "Step 400, Loss: 0.0512, Accuracy: 0.9868\n",
            "Step 500, Loss: 0.0342, Accuracy: 0.9869\n",
            "Step 600, Loss: 0.0137, Accuracy: 0.9864\n",
            "Step 700, Loss: 0.0461, Accuracy: 0.9866\n",
            "Step 800, Loss: 0.0060, Accuracy: 0.9867\n",
            "Step 900, Loss: 0.0309, Accuracy: 0.9866\n",
            "Step 1000, Loss: 0.1854, Accuracy: 0.9860\n",
            "Step 1100, Loss: 0.1569, Accuracy: 0.9856\n",
            "Step 1200, Loss: 0.0232, Accuracy: 0.9858\n",
            "Step 1300, Loss: 0.0410, Accuracy: 0.9859\n",
            "Step 1400, Loss: 0.0034, Accuracy: 0.9859\n",
            "Step 1500, Loss: 0.0334, Accuracy: 0.9862\n",
            "Step 1600, Loss: 0.0252, Accuracy: 0.9861\n",
            "Step 1700, Loss: 0.0096, Accuracy: 0.9861\n",
            "Step 1800, Loss: 0.0535, Accuracy: 0.9861\n",
            "Training Accuracy for epoch 5: 0.9861\n",
            "\n",
            "TF Training time: 43.09 seconds\n",
            "Test Accuracy: 0.9767\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Fill in normalization factor\n",
        "x_test = x_test / 255.0   # Fill in normalization factor\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),    # Fill size\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),    # Fill number of neurons and activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Fill number of neurons and activation\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "@tf.function  # compile the function into a graph\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch, training=True)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_acc_metric.update_state(y_batch, logits)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZY0oRj19Mx3C",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZY0oRj19Mx3C"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}